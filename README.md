# ElasticNet Regression Implementation

## Project Members
The following members worked on this project:

- Atharv Patil (A20580674)
- Emile Mondon (A########)
- Tejaswini Viswanath (A20536544)
- Merlin Santano Simoes (A20531255)
## Q1a. What does the Model do 
The ElasticNet Linear Regression model implemented in the above ipynb file is a combination of L1 (Lasso) and L2 (Ridge) regularization. The Model is implemented to address colinear Data and Overfitting problems of linear regression. The model is highly useful if the dataset has high dimensionaliity where features may be corelated. It can improve prediction accuracy by balancing btoh L1 and L2 Penalties.

### Q1b. When to Use
- **Multicollinearity**: When the Feature variables are highly corerelated to the target variabole, This model (ElasticNet) will help slecting a subset of Features and managing their coefficients effectively.
- **High-Dimensional Data**: It Can be use with dataset which has less samples but the number of features are high, Our Model (ElasticNet) gives us a approch to avoide overfitting while still fitting the model.
- **Variable Selection**: It can be used when the goal is to find the most important featurs from a large dataset, the L1 Penalty in the ElasticNet Model helps in varialble selection.

## Q2. Testing the Model
To determine if the model is working correctly, I followed a systematic testing approach:

1. **Colinear Data Generation**: The model was tested using colinear data generated by the `gen_data` function, ensuring that multicollinearity was present.
2. **Training and Testing**: The dataset was split into training and testing sets using the `train_test_split` method. The model was trained on the training set, and predictions were made on the separate test set.
3. **Performance Metrics**: The Mean Squared Error (MSE) was calculated on the test set to evaluate the accuracy of predictions. The goal was to achieve an MSE below 0.2.
4. **Monitoring Loss**: The loss (MSE) was printed at regular intervals during training to monitor convergence and ensure that the model was learning effectively.

## Q3. Tunable Parameters
The following parameters are exposed for tuning model performance:

- `alpha`: The regularization strength. Increasing this value increases the penalty on coefficients, which can reduce overfitting.
- `l1_ratio`: The mix ratio of L1 and L2 penalties. A value of 0.0 corresponds to L2 (Ridge) regression, while 1.0 corresponds to L1 (Lasso) regression. A value between 0 and 1 allows for a combination of both.
- `learning_rate`: The learning rate for gradient descent. This controls how much to change the model weights with respect to the loss gradient.
- `iterations`: The number of iterations for the gradient descent algorithm. Increasing this value may lead to better convergence but requires more computation time.

## Q4 Limitations and Future Work
The currently our Model may struggle with the following specific inputs:

- **Non-linear Data**: The model is inherently linear, which means it may not perform well on datasets that exhibit non-linear relationships. Given More time we could explore implementing polynomial features address this limitation.
- **Outliers**: The model's might not work well if significant outliers are present in the dataset, which can skew the loss calculation. Given more time more effective regression techniques could be exprimented to address this limitation.
- **Large Datasets**: The performance migh fall if we try when dealing with very large datasets due to computational cost. Given time experementing the use more efficient optimization algorithms like mini batch Gradient Descent could be give us positive results.

## Results
Here are the results from the model after training:

- **Learned Weights**: 
    ```
    [ 0.15523119  1.31573842  0.01273775  0.01404976  0.00763343 
    0.6889651  -2.38218345  1.6852422   0.9152097  -0.74469009]
    ```
- **Learned Bias**: 
    ```
    0.08460285521534855
    ```
- **Mean Squared Error on Test Set**: 
    ```
    0.269392645589893
    ```

### Visualization of Results
The following plot illustrates the comparison between actual and predicted values:

![Actual vs Predicted Values Comparison](https://github.com/AtharvPat/ML-Project_-1-/blob/main/Results/output.png)



## Usage
To use the ElasticNet regression model, follow these steps:

1. **Generate or Load Data**: You can use the `generate_data` method to create synthetic collinear data or load your own dataset.
2. **Split Data**: Use the `train_test_split` method to divide your dataset into training and testing sets.
3. **Standardize Data**: The `standardize_data` method standardizes the feature sets for better performance during training.
4. **Fit the Model**: Call the `fit` method with the training data to train the model.
5. **Make Predictions**: Use the `predict` method to generate predictions on the test set.
6. **Evaluate the Model**: Calculate metrics such as Mean Squared Error (MSE) to assess the model's performance.

### Example
```python
X, y = Gen_data(1000, 10)


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)


X_train_std, X_test_std = standardize(X_train, X_test)


model = ElasticNet(alpha = 0.01,
                   l_ratio = 0.2,
                   learning_rate =0.0005,
                   iterations= 10000)


model.fit(X_train_std.values, y_train.values)


y_pred =  model.predict(X_test_std.values)
